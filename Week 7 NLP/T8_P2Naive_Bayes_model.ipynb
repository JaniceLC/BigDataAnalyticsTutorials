{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T8_P2Naive_Bayes_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nN--SGi6wSUD"},"source":["# Training a Naive Bayes model to identify the author of an email or document"]},{"cell_type":"markdown","metadata":{"id":"FyUz8IR-wd_y"},"source":["In this example, we use a set of emails or documents that were written by two different individuals. The purpose is to train a Naive Bayes model to be able to predict who wrote a document/email, given the words used in it\n","The Github repository with the files used in this example can be found [here](https://github.com/ivanddm/naivebayes-email-author). The file ```nb_email_author.py``` contains the script that loads the data, trains the model, and find the score of the prediction for train and test sets.\n","Here, we explain the main parts of the script and the results."]},{"cell_type":"markdown","metadata":{"id":"e8EmqZTKwpC5"},"source":["## Loading the data\n","The first section of the script loads the data. The only particularity here is that ‘pickle’ is used deserialize the original data file (for more on pickle and serialization/deserialization, take a look at this  [link](https://docs.python.org/3/library/pickle.html)).\n","Two files are loaded, one that contains the emails or documents, and another one with just the emails’ authors. Once we deserialize and load the files, we have two arrays (lists), one named ```words```, and one named ```authors```, each of size 17,578. Each element in ```words``` is a single string that contains an email or document. Each element in `authors` is either 0 or 1.\n","As usual, we split the data into train and test sets using the Scikit-learn method `sklearn.model_selection.train_test_split`."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wr--ie1VKzZ-","executionInfo":{"status":"ok","timestamp":1649002661893,"user_tz":-480,"elapsed":2695,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}},"outputId":"dfa9e593-f8fd-48a5-a223-44c941bc96b2"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/gdrive/MyDrive/STAT4609_2021/'week8_NLP Basic NLP'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ojl-f95DLDlI","executionInfo":{"status":"ok","timestamp":1649002661893,"user_tz":-480,"elapsed":9,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}},"outputId":"769a2933-1542-49b7-911f-ee5d3610acd8"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/STAT4609_2021/week8_NLP Basic NLP\n"]}]},{"cell_type":"code","metadata":{"id":"EDOTjq0rSCE2","executionInfo":{"status":"ok","timestamp":1649002662387,"user_tz":-480,"elapsed":501,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["import pickle\n","from time import time\n","import pandas as pd \n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection import SelectPercentile, f_classif\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Loading the Data\n","\n","\n","words_file = \"./word_data.pkl\"\n","authors_file = \"./email_authors.pkl\"\n","\n","authors_file_handler = open(authors_file, \"rb\")\n","authors = pickle.load(authors_file_handler)\n","authors_file_handler.close()\n","\n","words_file_handler = open(words_file, \"rb\")\n","words = pickle.load(words_file_handler)\n","words_file_handler.close()\n"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"up4OelS-wved","executionInfo":{"status":"ok","timestamp":1649002664095,"user_tz":-480,"elapsed":371,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["features_train, features_test, labels_train, labels_test = train_test_split(words, authors, test_size=0.1, random_state=10)"],"execution_count":49,"outputs":[]},{"cell_type":"code","source":["features_train[0:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"131Nvz0-N-zU","executionInfo":{"status":"ok","timestamp":1649002690176,"user_tz":-480,"elapsed":4,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}},"outputId":"933d0ea9-c0c9-47c4-a125-60364cad2c3d"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['  second messag from bruce forward by  houect on 01122000 0444 pm enron north america corp from mari heard 01122000 0428 pm to  houectect cc subject re brokerag accountciesa transact fyi forward by mari heardhouect on 01122000 0428 pm bruce harrisenrondevelop 01122000 0352 pm to nanci muchmoreenrondevelopmentenrondevelop cc rick hopkinsonenrondevelopmentenrondevelop andrea bertoneenrondevelopmentenrondevelop glenn e matthysenrondevelopmentenrondevelop mari heardect subject re brokerag accountciesa transact i do not know who should buy the share rick and glenn how do you want the share to be purchas and where do you want them own keep in mind that if epca buy the share and then sell them same day edid for exampl we will i believ need to open 2 account in addit if the share trade over the exchang i am not sure exact how we will execut a privat sale between those 2 compani it seem to me that you may want to have the ultim owner actual make the purchas and then tri to structur your cashflow to accomplish your tax goal rather than to accomplish those goal by move around the share we can talk about this tomorrow but the sooner we have the structur in place the sooner the account can be open we will need to coordin this structur with corp treasuri cathi moehlmann and robin veariel regard bruce nanci muchmor 01122000 0225 pm to bruce harrisenrondevelopmentenrondevelop rick hopkinsonenrondevelopmentenrondevelop cc andrea bertoneenrondevelopmentenrondevelop glenn e matthysenrondevelopmentenrondevelop mari heardect subject brokerag accountciesa transact pleas let me know which compani need to open a brokerag account in connect with the ciesa transact onc you tell me which compani it should be we can start the corpor author process and i will coordin with mari heard of ena concern other inform shell need in connect with open the account pleas bear in mind that it will take some time im not certain exact how longi will find out to get all of the proper author and other paper work into place onc you have decid on the compani nanci',\n"," '  stewart ill have to review their master to be certain and insur that there is no specif prohibit against weather transact in general there is noth els requir specif although we may have to add some languag to the weather confirm themselv  stewart rosman 06062000 0526 pm to  houectect cc subject merc question  doe merc need to do anyth to enter into financi weather transact rememb that they have a isda and have done numer power swap for the past year thank stewart',\n"," '  tanya per my voic mail is ani type of corpor guaranti requir from this counterparti doe our addit event of default requir ani chang    enron north america corp 1400 smith street eb 3801a houston texa 77002 7138535620 phone 7136463490 fax enroncom']"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"QfVp4yVGw8Zk"},"source":["## Text vectorization\n","When dealing with texts in machine learning, it is quite common to transform the text into data that can be easily analyzed and quantify.\n","For this, the most commonly used technique is the **tf-idf** short for “term frequency-inverse document frequency”, which basically reflects how important a word is to a document (email) in a collection or corpus (our set of emails or documents).\n","The **tf-idf** is an statistic that increases with the number of times a word appears in the document, penalized by the number of documents in the corpus that contain the word ([Wikipedia](https://en.wikipedia.org/wiki/Tf–idf)).\n","Fortunately for us, Scikit-learn has a method that does just this (`sklearn.feature_extraction.text.TfidfVectorizer`). See the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)).\n","So we apply this method to our data in the following way:\n","\n"]},{"cell_type":"code","metadata":{"id":"WnpAsGmTxO1L","executionInfo":{"status":"ok","timestamp":1649002774094,"user_tz":-480,"elapsed":2367,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n","features_train = vectorizer.fit_transform(features_train)\n","features_test = vectorizer.transform(features_test)"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VpRUZaRxNmE"},"source":["`TfidfVectorizer` sets the vectorizer up. Here we change `sublinear_tf` to true, which replaces **tf** with **1 + log(tf)**. This addresses the issue that “twenty occurrences of a term in a document” does not represent “twenty times the significance of a single occurrence” ([link](https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html)). Therefore, it reduces the importance of high frequency words (note that $1+log(1) = 1$, while $1+log(20) = 2.3$).\n","Additionally, `stop_words` is set to ‘english’, so stop words such as “and”, “the”, “him” will be ignored in this case, and `max_df=0.5` means that we are ignoring terms that have a document frequency higher than 0.5 (i.e., the proportion of documents in which the term is found).\n","Next, we fit and transform the features (terms or words in our case) for both train and test sets. Notice that for the train set we use `fit_transform`, and for the test set we use just `transform`.\n","This makes sense since we want the model to learn the vocabulary and the document frequencies by the train set, and then transform the train features into a terms-document matrix. For the test set we just want to use the learned document frequencies (idf’s) and vocabulary to transform it into a term-document matrix.\n","\n","## Let’s see how this look like with a simplified example:\n","Suppose we have the following train corpus, again, each item represents one document/email:"]},{"cell_type":"code","metadata":{"id":"qHDQmyEGxgvt","executionInfo":{"status":"ok","timestamp":1649002774094,"user_tz":-480,"elapsed":8,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["corpus = [\n","    \"This is my first email.\",\n","    \"I'm trying to learn machine learning.\",\n","    \"This is the second email\",\n","    \"Learning is fun\"\n","]"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQcmUrjCxjSx"},"source":["Now, let’s fit and transform it:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mYi2Ebexl1y","outputId":"91bcdff7-88e7-460c-e8eb-5f6cf04f1b23","executionInfo":{"status":"ok","timestamp":1649002774095,"user_tz":-480,"elapsed":9,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","print(X.__str__)\n","# <4x13 sparse matrix of type ‘<class ‘numpy.float64’>’ with 18 stored elements in Compressed Sparse Row format>"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method spmatrix.__str__ of <4x13 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 18 stored elements in Compressed Sparse Row format>>\n"]}]},{"cell_type":"markdown","metadata":{"id":"tJGmbpgwxoAs"},"source":["`fit_transform` returns a sparse matrix:"]},{"cell_type":"code","metadata":{"id":"uwg_rkdVxqS6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ba78b6f-da9b-4bab-f82e-ec4e58d6b871","executionInfo":{"status":"ok","timestamp":1649002774989,"user_tz":-480,"elapsed":379,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["print(X)"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 0)\t0.41263976171812644\n","  (0, 1)\t0.5233812152405496\n","  (0, 7)\t0.5233812152405496\n","  (0, 3)\t0.3340674500232949\n","  (0, 10)\t0.41263976171812644\n","  (1, 5)\t0.3667390112974172\n","  (1, 6)\t0.4651619335222394\n","  (1, 4)\t0.4651619335222394\n","  (1, 11)\t0.4651619335222394\n","  (1, 12)\t0.4651619335222394\n","  (2, 8)\t0.5233812152405496\n","  (2, 9)\t0.5233812152405496\n","  (2, 0)\t0.41263976171812644\n","  (2, 3)\t0.3340674500232949\n","  (2, 10)\t0.41263976171812644\n","  (3, 2)\t0.7020348194149619\n","  (3, 5)\t0.5534923152870045\n","  (3, 3)\t0.4480997313625986\n"]}]},{"cell_type":"markdown","metadata":{"id":"idu08GywxtGE"},"source":["If we transform `X`into a 2D array, it looks like this (there are 13 columns in total, each represent a word/term, odd columns are omitted for brevity):"]},{"cell_type":"code","metadata":{"id":"krnk4lE8xw4I","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"74de7351-3068-43ae-8eea-c52801cb373e","executionInfo":{"status":"ok","timestamp":1649002776037,"user_tz":-480,"elapsed":6,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["vocabulary = vectorizer.get_feature_names()\n","pd.DataFrame(data=X.toarray(), columns=vocabulary).iloc[:,0::2]"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["     email       fun     learn   machine    second     this    trying\n","0  0.41264  0.000000  0.000000  0.000000  0.000000  0.41264  0.000000\n","1  0.00000  0.000000  0.465162  0.465162  0.000000  0.00000  0.465162\n","2  0.41264  0.000000  0.000000  0.000000  0.523381  0.41264  0.000000\n","3  0.00000  0.702035  0.000000  0.000000  0.000000  0.00000  0.000000"],"text/html":["\n","  <div id=\"df-69fcb504-d615-4ba1-bd3f-c8e02b2ad992\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>email</th>\n","      <th>fun</th>\n","      <th>learn</th>\n","      <th>machine</th>\n","      <th>second</th>\n","      <th>this</th>\n","      <th>trying</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.41264</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.41264</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.465162</td>\n","      <td>0.465162</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.465162</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.41264</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.523381</td>\n","      <td>0.41264</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.00000</td>\n","      <td>0.702035</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69fcb504-d615-4ba1-bd3f-c8e02b2ad992')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-69fcb504-d615-4ba1-bd3f-c8e02b2ad992 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-69fcb504-d615-4ba1-bd3f-c8e02b2ad992');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"sHOSUC75x0T1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3aab0dc5-b166-47a9-844a-a2858d7bad13","executionInfo":{"status":"ok","timestamp":1649002776891,"user_tz":-480,"elapsed":3,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["print(vocabulary)\n","# [‘email’, ‘first’, ‘fun’, ‘is’, ‘learn’, ‘learning’, ‘machine’, ‘my’, ‘second’, ‘the’, ‘this’, ‘to’, ‘trying’]"],"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["['email', 'first', 'fun', 'is', 'learn', 'learning', 'machine', 'my', 'second', 'the', 'this', 'to', 'trying']\n"]}]},{"cell_type":"markdown","metadata":{"id":"604n23G7x3PZ"},"source":["Now let’s suppose that we have the following ‘test’ document:"]},{"cell_type":"code","metadata":{"id":"8T9AAOzlx67l","executionInfo":{"status":"ok","timestamp":1649002778192,"user_tz":-480,"elapsed":3,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["test = [\"I’m also trying to learn python\"]"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XX38sljjx9Oa"},"source":["Let’s transform it and take a look at it:"]},{"cell_type":"code","metadata":{"id":"9Tf-02rUyAgn","colab":{"base_uri":"https://localhost:8080/","height":81},"outputId":"eed0b848-7c13-44e1-b66d-0ea0e0e71308","executionInfo":{"status":"ok","timestamp":1649002779577,"user_tz":-480,"elapsed":6,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["X_test = vectorizer.transform(test)\n","pd.DataFrame(data=X_test.toarray(), columns=vocabulary).iloc[:, 0::2]"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   email  fun    learn  machine  second  this   trying\n","0    0.0  0.0  0.57735      0.0     0.0   0.0  0.57735"],"text/html":["\n","  <div id=\"df-7e38d10d-7fb2-467a-a876-4d82849838bb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>email</th>\n","      <th>fun</th>\n","      <th>learn</th>\n","      <th>machine</th>\n","      <th>second</th>\n","      <th>this</th>\n","      <th>trying</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.57735</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.57735</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e38d10d-7fb2-467a-a876-4d82849838bb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7e38d10d-7fb2-467a-a876-4d82849838bb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7e38d10d-7fb2-467a-a876-4d82849838bb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"GnMqFLAWyC4g"},"source":["There you have it, this is how texts or documents are vectorized for further analysis.\n","\n","## Selecting a smaller set of features\n","Although selecting a smaller set of features is not strictly necessary, it could be computationally challenging to train a model with too many words or features.\n","In this example, we use Scikit-learn’s `SelectPercentile` to choose features with highest scores ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html)):\n","\n","`f_classif`\n","\n","Compute the ANOVA F-value for the provided sample.\n","F-value between label/feature is calculated via univariate linear regression tests.\n","\n"]},{"cell_type":"code","metadata":{"id":"RRdghuBeyH6Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649002810406,"user_tz":-480,"elapsed":452,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}},"outputId":"82446d5c-e7d1-473e-fb96-7cd740af7c44"},"source":["from sklearn.feature_selection import SelectPercentile, f_classif\n","selector = SelectPercentile(f_classif, percentile=10)\n","selector.fit(features_train, labels_train)\n","features_train = selector.transform(features_train).toarray()\n","features_test = selector.transform(features_test).toarray()\n","features_test"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"UcMs5GtKyKC0"},"source":["The selector uses `f_classif` as score function, which computes the ANOVA F-values for the sample. Basically we are choosing the terms with largest F-values (i.e. terms or words for which the frequency mean is most likely to be different across classes or authors). This is common in order to choose the best discriminatory features across classes (out of 38,209 words initially, we end up with 3,821).\n","\n","##Training a Naive Bayes model\n","For this example, we use a Gaussian Naive Bayes (NB) implementation (Scikit-learn documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)). In future articles, we will discuss in detail the theory behind Naive Bayes.\n","For now, it is worth saying that NB is based on applying the Bayes’ rule to calculate the probability or likelihood that a set of words (document/email) is written by someone or some class (e.g. **P(“Chris”| “learn”, “machine”, “trying”,…)**).\n","However, there is no such a thing as a naive Bayes’ rule. The ‘naive’ term arises due to assuming that features are independent from each other (conditional independence), which means, for our emails analysis, that we are assuming that the location of words in a sentence is completely random (i.e. ‘am’ or ‘robot’ are equally likely to follow the word ‘I’, which of course is not true).\n","##NB with Scikit-learn\n","In general, training machine learning models with Scikit-learn is straightforward and it normally follows the same pattern:\n","\n","* initialize an instance of the class model,\n","* fit the train data,\n","* predict the test data (we omit that here),\n","* compute scores for both train and test sets.\n","\n","The scores here refers to the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy, which is a harsh metric since you require for each sample that each label set be correctly predicted."]},{"cell_type":"code","metadata":{"id":"dN1erL7Cymnn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e953a47-2fe0-4874-90dc-896ca9f2d3c2","executionInfo":{"status":"ok","timestamp":1649002139997,"user_tz":-480,"elapsed":2024,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from time import time\n","t0 = time()\n","model = GaussianNB()\n","model.fit(features_train, labels_train)\n","print(f\"\\nTraining time: {round(time()-t0, 3)}s\")\n","t0 = time()\n","score_train = model.score(features_train, labels_train)\n","print(f\"Prediction time (train): {round(time()-t0, 3)}s\")\n","t0 = time()\n","score_test = model.score(features_test, labels_test)\n","print(f\"Prediction time (test): {round(time()-t0, 3)}s\")\n","print(\"\\nTrain set score:\", score_train)\n","print(\"Test set score:\", score_test)"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training time: 0.764s\n","Prediction time (train): 0.746s\n","Prediction time (test): 0.081s\n","\n","Train set score: 0.9785082174462706\n","Test set score: 0.9783845278725825\n"]}]},{"cell_type":"markdown","metadata":{"id":"aWPnyJWzVbcz"},"source":["##Training a Logistic Regression Model \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tY8yqtJkUaA0","outputId":"aa4b347d-448a-42f5-ff05-138a4b2c9455","executionInfo":{"status":"ok","timestamp":1649002345595,"user_tz":-480,"elapsed":7402,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["from sklearn.linear_model import LogisticRegression\n","t0 = time()\n","clf = LogisticRegression(random_state=0).fit(features_train, labels_train)\n","print(f\"\\nTraining time:: {round(time()-t0, 3)}s\")\n","t0 = time()\n","score_train = clf.score(features_train, labels_train)\n","print(f\"Prediction time (train): {round(time()-t0, 3)}s\")\n","t0 = time()\n","score_test = clf.score(features_test, labels_test)\n","print(f\"Prediction time (test): {round(time()-t0, 3)}s\")\n","print(\"\\nTrain set score:\", score_train)\n","print(\"Test set score:\", score_test)"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training time:: 6.823s\n","Prediction time (train): 0.147s\n","Prediction time (test): 0.019s\n","\n","Train set score: 0.9843868520859671\n","Test set score: 0.9795221843003413\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUDiLo7rWYbV","outputId":"4c47229a-0c00-440a-e0c5-106ec757950d","executionInfo":{"status":"ok","timestamp":1649002345596,"user_tz":-480,"elapsed":8,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["pred_y_test = clf.predict(features_test)\n","pred_y_test"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, ..., 0, 0, 1])"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"sIN4nhnbWwYl"},"source":["By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n","\n","Thus in binary classification, the count of true negatives is $C_{0,0,}$ false negatives is $C_{1,0}$ true positives is $C_{1,1}$ and false positives is $C_{0,1}$."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lT-VNC39WMD1","outputId":"02fc7fae-87e1-445c-d387-cbc8b0c58903","executionInfo":{"status":"ok","timestamp":1649002345597,"user_tz":-480,"elapsed":7,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix(labels_test, pred_y_test)"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[842,  24],\n","       [ 12, 880]])"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"kXYQZiqZWyHY"},"source":["### Exercise:\n","\n","Do you know how to calculate the recall, precision, F1 score for evaluating the model performance on the test set. \n"]}]}