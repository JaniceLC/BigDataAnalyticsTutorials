{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T9_3_Perceptron_and_neural_networks.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"cells":[{"cell_type":"code","metadata":{"id":"KKxtakW3P-VF","executionInfo":{"status":"ok","timestamp":1649608640362,"user_tz":-480,"elapsed":9,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_yf5OtVWP-VG"},"source":["\n","Neural Networks\n","===============\n","\n","Neural networks can be constructed using the ``torch.nn`` package.\n","\n","Now that you had a glimpse of ``autograd``, ``nn`` depends on\n","``autograd`` to define models and differentiate them.\n","An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n","returns the ``output``.\n","\n","For example, look at perceptron:\n","\n","\n","It is a simple feed-forward network. It takes the input, feeds it\n","through several layers one after the other, and then finally gives the\n","output.\n","\n","A typical training procedure for a neural network is as follows:\n","\n","- Define the neural network that has some learnable parameters (or\n","  weights)\n","- Iterate over a dataset of inputs\n","- Process input through the network\n","- Compute the loss (how far is the output from being correct)\n","- Propagate gradients back into the network’s parameters\n","- Update the weights of the network, typically using a simple update rule:\n","  ``weight = weight - learning_rate * gradient``\n","\n","Define the network, Single Layer Perceptron\n","------------------\n","\n"]},{"cell_type":"code","metadata":{"id":"U8b_oAjOP-VH","executionInfo":{"status":"ok","timestamp":1649608647403,"user_tz":-480,"elapsed":7047,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Perceptron(nn.Module):\n","    def __init__(self, input_size, num_class):\n","        super(Perceptron, self).__init__()\n","        self.fc = nn.Linear(input_size, num_class)\n","        self.sigmoid = torch.nn.Sigmoid() \n","        # use sigmoid function as the activation function\n","    def forward(self, x):\n","        output = self.fc(x)\n","        output = self.sigmoid(output) \n","        return output"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcwFBs8QiSAB","executionInfo":{"status":"ok","timestamp":1649608647407,"user_tz":-480,"elapsed":16,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["??nn.Linear"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tloiaTCT0vr"},"source":["Note that class ```nn.Linear(Module)``` applies a linear transformation to the incoming data: :math:$$y = xA^T + b$$\n"]},{"cell_type":"markdown","metadata":{"id":"mSBahoHxP-VI"},"source":["You just have to define the ``forward`` function, and the ``backward``\n","function (where gradients are computed) is automatically defined for you\n","using ``autograd``.\n","You can use any of the Tensor operations in the ``forward`` function.\n","\n","The learnable parameters of a model are returned by ``net.parameters()``\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iCU4LFZP-VI","outputId":"ccdb5c65-8db9-451c-c3b7-5259f9642987","executionInfo":{"status":"ok","timestamp":1649608647439,"user_tz":-480,"elapsed":44,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["params = list(Perceptron(input_size=48, num_class=10).parameters())\n","print(len(params))\n","print(params[0].size())  \n","print(params[1].size())  "],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","torch.Size([10, 48])\n","torch.Size([10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"snB6z2zLP-VI"},"source":["*Let*'s try a random 48 input.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSm6q72pP-VJ","outputId":"64faefd4-05da-4bdb-c40a-41c666b63015","executionInfo":{"status":"ok","timestamp":1649608648074,"user_tz":-480,"elapsed":674,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["input = torch.randn(48)\n","print(input.size())\n","model = Perceptron(input_size=48, num_class=10)\n","out = model(input)\n","print(out.size())"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([48])\n","torch.Size([10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"6WuPIJ_jP-VJ"},"source":["Zero the gradient buffers of all parameters and backprops with random\n","gradients:\n","\n"]},{"cell_type":"code","metadata":{"id":"gYF93MyiP-VJ","executionInfo":{"status":"ok","timestamp":1649608648075,"user_tz":-480,"elapsed":21,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["model.zero_grad()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZbbC09IP-VJ"},"source":["<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n","    package only supports inputs that are a mini-batch of samples, and not\n","    a single sample.\n","\n","    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n","    ``nSamples x nChannels x Height x Width``.\n","\n","    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n","    a fake batch dimension.</p></div>\n","\n","Before proceeding further, let's recap all the classes you’ve seen so far.\n","\n","**Recap:**\n","  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n","     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n","     tensor.\n","  -  ``nn.Module`` - Neural network module. *Convenient way of\n","     encapsulating parameters*, with helpers for moving them to GPU,\n","     exporting, loading, etc.\n","  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n","     registered as a parameter when assigned as an attribute to a*\n","     ``Module``.\n","  -  ``autograd.Function`` - Implements *forward and backward definitions\n","     of an autograd operation*. Every ``Tensor`` operation creates at\n","     least a single ``Function`` node that connects to functions that\n","     created a ``Tensor`` and *encodes its history*.\n","\n","**At this point, we covered:**\n","  -  Defining a neural network\n","  -  Processing inputs and calling backward\n","\n","**Still Left:**\n","  -  Computing the loss\n","  -  Updating the weights of the network\n","\n","Loss Function\n","-------------\n","A loss function takes the (output, target) pair of inputs, and computes a\n","value that estimates how far away the output is from the target.\n","\n","There are several different\n","`loss functions <https://pytorch.org/docs/nn.html#loss-functions>`_ under the\n","nn package .\n","A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n","between the input and the target.\n","\n","For example:\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMYVihs0P-VK","outputId":"2379a6fa-f994-4f52-dc06-6cfbc94c140b","executionInfo":{"status":"ok","timestamp":1649608648076,"user_tz":-480,"elapsed":20,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["output = model(input)\n","target = torch.randn(10)  # a dummy target, for example\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.0273, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"roAtnFngP-VK"},"source":["Now, if you follow ``loss`` in the backward direction, using its\n","``.grad_fn`` attribute, you will see a graph of computations that looks\n","like this:\n","\n","So, when we call ``loss.backward()``, the whole graph is differentiated\n","w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n","will have their ``.grad`` Tensor accumulated with the gradient.\n","\n","For illustration, let us follow a few steps backward:\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8-EGBG0P-VL","outputId":"09916143-b4e1-4b2a-f9b9-77467851aeb3","executionInfo":{"status":"ok","timestamp":1649608648076,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["print(loss.grad_fn)  # MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["<MseLossBackward0 object at 0x7f7ad7727c90>\n","<SigmoidBackward0 object at 0x7f7ad77271d0>\n","<AddBackward0 object at 0x7f7ad7727c10>\n"]}]},{"cell_type":"markdown","metadata":{"id":"ktM1AUnZP-VL"},"source":["Backprop\n","--------\n","To backpropagate the error all we have to do is to ``loss.backward()``.\n","You need to clear the existing gradients though, else gradients will be\n","accumulated to existing gradients.\n","\n","\n","Now we shall call ``loss.backward()``, and have a look at conv1's bias\n","gradients before and after the backward.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7nZQhevP-VL","outputId":"7ce3c4d1-a790-4ff9-e002-943868258a10","executionInfo":{"status":"ok","timestamp":1649608648076,"user_tz":-480,"elapsed":10,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["model.zero_grad()     # zeroes the gradient buffers of all parameters\n","\n","print('fc.bias.grad before backward')\n","print(model.fc.bias.grad)\n","\n","loss.backward()\n","\n","print('fc.bias.grad after backward')\n","print(model.fc.bias.grad)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["fc.bias.grad before backward\n","None\n","fc.bias.grad after backward\n","tensor([-0.0067, -0.0137,  0.0287,  0.0041, -0.1154, -0.0769,  0.0350,  0.0452,\n","        -0.0260, -0.0090])\n"]}]},{"cell_type":"markdown","metadata":{"id":"AR28AoaaP-VM"},"source":["Now, we have seen how to use loss functions.\n","\n","**Read Later:**\n","\n","  The neural network package contains various modules and loss functions\n","  that form the building blocks of deep neural networks. A full list with\n","  documentation is `here <https://pytorch.org/docs/nn>`_.\n","\n","**The only thing left to learn is:**\n","\n","  - Updating the weights of the network\n","\n","Update the weights\n","------------------\n","The simplest update rule used in practice is the Stochastic Gradient\n","Descent (SGD):\n","\n","     ``weight = weight - learning_rate * gradient``\n","\n","We can implement this using simple Python code:\n","\n",".. code:: python\n","\n","    learning_rate = 0.01\n","    for f in net.parameters():\n","        f.data.sub_(f.grad.data * learning_rate)\n","\n","However, as you use neural networks, you want to use various different\n","update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n","To enable this, we built a small package: ``torch.optim`` that\n","implements all these methods. Using it is very simple:\n","\n"]},{"cell_type":"code","metadata":{"id":"ochYMG8VP-VN","executionInfo":{"status":"ok","timestamp":1649608648077,"user_tz":-480,"elapsed":7,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["import torch.optim as optim\n","\n","# create your optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# in your training loop:\n","optimizer.zero_grad()   # zero the gradient buffers\n","output = model(input)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step()    # Does the update"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1RRK9HyP-VN"},"source":[".. Note::\n","\n","      Observe how gradient buffers had to be manually set to zero using\n","      ``optimizer.zero_grad()``. This is because gradients are accumulated\n","      as explained in the `Backprop`_ section.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NlpBEod2XMWo"},"source":["## For Multilayer Perceptron\n","For this example, I only put one hidden layer but you can add as many hidden layers as you want. When you have more than two hidden layers, the model is also called the deep/multilayer feedforward model or multilayer perceptron model(MLP).\n","\n","\n","After the hidden layer, I use ReLU as activation before the information is sent to the output layer. This is to introduce non-linearity to the linear output from the hidden layer as mentioned earlier. What ReLU does here is that if the function is applied to a set of numerical values, any negative value will be converted to 0 otherwise the values stay the same. For example, if the input set is [-1,0,4,-5,6] then the function will return [0,0,4,0,6].\n","\n","As an output activation function, I used Sigmoid. This is because the example I want to show you later is a binary classification task, meaning we have binary categories to predict from. Sigmoid is the good function to use because it calculates the probability(ranging between 0 and 1) of the target output being label 1. As said in the previous section, the choice of the activation function depends on your task. Now, let’s see a binary classifier example using this model.\n","\n"]},{"cell_type":"code","metadata":{"id":"Uk-JmKkqXLZS","executionInfo":{"status":"ok","timestamp":1649608648077,"user_tz":-480,"elapsed":6,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["class Feedforward(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(Feedforward, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size  = hidden_size\n","        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n","        self.relu = torch.nn.ReLU()\n","        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n","        self.sigmoid = torch.nn.Sigmoid()\n","    def forward(self, x):\n","        hidden = self.fc1(x)\n","        relu = self.relu(hidden)\n","        output = self.fc2(relu)\n","        output = self.sigmoid(output)\n","        return output"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"g10fnnABj9M4","executionInfo":{"status":"ok","timestamp":1649608648769,"user_tz":-480,"elapsed":698,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["# CREATE RANDOM DATA POINTS\n","import numpy as np\n","from sklearn.datasets import make_blobs\n","def blob_label(y, label, loc): # assign labels\n","    target = np.copy(y)\n","    for l in loc:\n","        target[y == l] = label\n","    return target\n","x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n","x_train = torch.FloatTensor(x_train)\n","y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n","y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n","x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n","x_test = torch.FloatTensor(x_test)\n","y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n","y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Udpmd5uj_Wy","executionInfo":{"status":"ok","timestamp":1649608648769,"user_tz":-480,"elapsed":9,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["model = Feedforward(2, 10)\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTcAPjznkBI7","outputId":"32e9a9ba-66ec-4092-cb93-6418aab79898","executionInfo":{"status":"ok","timestamp":1649608648770,"user_tz":-480,"elapsed":7,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["model.eval()\n","y_pred = model(x_test)\n","before_train = criterion(y_pred.squeeze(), y_test)\n","print('Test loss before training' , before_train.item())"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss before training 0.715816855430603\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BretOaFUkNqw","outputId":"da768447-0927-4cf2-a87c-631bac6d9a81","executionInfo":{"status":"ok","timestamp":1649608649502,"user_tz":-480,"elapsed":737,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["model.train()\n","epoch = 50\n","loss_hist = []\n","for epoch in range(epoch):\n","    optimizer.zero_grad()\n","    # Forward pass\n","    y_pred = model(x_train)\n","    # Compute Loss\n","    loss = criterion(y_pred.squeeze(), y_train)\n","   \n","    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n","    # Backward pass\n","    loss.backward()\n","    optimizer.step()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: train loss: 0.4507593512535095\n","Epoch 1: train loss: 0.4389355778694153\n","Epoch 2: train loss: 0.42835408449172974\n","Epoch 3: train loss: 0.41887903213500977\n","Epoch 4: train loss: 0.4103502333164215\n","Epoch 5: train loss: 0.40255647897720337\n","Epoch 6: train loss: 0.3953876495361328\n","Epoch 7: train loss: 0.388751357793808\n","Epoch 8: train loss: 0.3825705349445343\n","Epoch 9: train loss: 0.3767983317375183\n","Epoch 10: train loss: 0.3714091181755066\n","Epoch 11: train loss: 0.36629945039749146\n","Epoch 12: train loss: 0.3614397346973419\n","Epoch 13: train loss: 0.3567997217178345\n","Epoch 14: train loss: 0.3523494601249695\n","Epoch 15: train loss: 0.34807300567626953\n","Epoch 16: train loss: 0.3439566493034363\n","Epoch 17: train loss: 0.33998075127601624\n","Epoch 18: train loss: 0.3361317217350006\n","Epoch 19: train loss: 0.3323979079723358\n","Epoch 20: train loss: 0.3287695050239563\n","Epoch 21: train loss: 0.32523784041404724\n","Epoch 22: train loss: 0.32179561257362366\n","Epoch 23: train loss: 0.3184363842010498\n","Epoch 24: train loss: 0.3151546120643616\n","Epoch 25: train loss: 0.3119385838508606\n","Epoch 26: train loss: 0.3087828457355499\n","Epoch 27: train loss: 0.30569130182266235\n","Epoch 28: train loss: 0.3026605546474457\n","Epoch 29: train loss: 0.29968753457069397\n","Epoch 30: train loss: 0.29676949977874756\n","Epoch 31: train loss: 0.29390397667884827\n","Epoch 32: train loss: 0.2910887598991394\n","Epoch 33: train loss: 0.2883218824863434\n","Epoch 34: train loss: 0.2856013774871826\n","Epoch 35: train loss: 0.28292614221572876\n","Epoch 36: train loss: 0.2802952826023102\n","Epoch 37: train loss: 0.27770599722862244\n","Epoch 38: train loss: 0.27515697479248047\n","Epoch 39: train loss: 0.2726469933986664\n","Epoch 40: train loss: 0.2701748311519623\n","Epoch 41: train loss: 0.26773953437805176\n","Epoch 42: train loss: 0.26534003019332886\n","Epoch 43: train loss: 0.26297539472579956\n","Epoch 44: train loss: 0.2606446444988251\n","Epoch 45: train loss: 0.2583447992801666\n","Epoch 46: train loss: 0.2560792565345764\n","Epoch 47: train loss: 0.2538653314113617\n","Epoch 48: train loss: 0.2516799569129944\n","Epoch 49: train loss: 0.24952097237110138\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZLTUfeudlYFZ"},"source":["Let’s start training. First I switch the module mode to ```.train()``` so that new weights can be learned after every ```epoch. optimizer.zero_grad()``` sets the gradients to zero before we start backpropagation. This is a necessary step as PyTorch accumulates the gradients from the backward passes from the previous epochs.\n","After the forward pass and the loss computation, we perform backward pass by calling ```loss.backward()```, which computes the gradients. Then ```optimizer.step()``` updates the weights accordingly.\n","\n","\n","### Evaluation\n","Okay, the training is now done. Let’s see how the test loss changed after the training. Again, we switch the module mode back to the evaluation mode and check the test loss as the example below.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9B7EFRRVkQ5d","outputId":"d3a559d0-84af-46b3-9169-3d73aa34f664","executionInfo":{"status":"ok","timestamp":1649608649503,"user_tz":-480,"elapsed":14,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["model.eval()\n","y_pred = model(x_test)\n","after_train = criterion(y_pred.squeeze(), y_test) \n","print('Test loss after Training' , after_train.item())"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss after Training 0.7768269777297974\n"]}]},{"cell_type":"markdown","metadata":{"id":"4KD5oQWclTYv"},"source":["In order to improve the model, you can try out different parameter values for your hyperparameters(ie. hidden dimension size, epoch size, learning rates). You can also try changing the structure of your model (ie. adding more hidden layers) to see if your mode improves. There is a number of different hyperparameter and model selection techniques popularly used but this is the general idea behind it. In the end, you can select the hyperparameters and the model structure that gives you the best performance."]},{"cell_type":"markdown","metadata":{"id":"C3GXKVsmlXiw"},"source":[""]}]}