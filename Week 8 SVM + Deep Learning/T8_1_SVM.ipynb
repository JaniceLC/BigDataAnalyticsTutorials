{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"T9_1_SVM.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"em-2oTiXBJmj"},"source":["\n","\n","# Support Vector Classifier \n","\n","I would like to mention that while there are many libraries/frameworks available to implement SVM (Support Vector Machine) algorithm without writing a bunch of code, I decided to write the code with as few high-level libraries as possible so that you and I can get a good grasp of important components involved in training an SVM model (with 99% accuracy, 0.98 recall, and precision). If you are looking for a quick implementation of SVM, then you are better off using packages like scikit-learn, cvxopt, etc.\n"]},{"cell_type":"code","metadata":{"id":"M97GwXQWBJmm","executionInfo":{"status":"ok","timestamp":1649650939754,"user_tz":-480,"elapsed":364,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["import numpy as np\n","import pandas as pd\n","import itertools\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","np.random.seed(1234)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CVrPz7jHBJmt"},"source":["## Fitting the data\n","Below, first we implement the `fit` function that learns the model parameters. The convex optimization problem for the SVM is equivalent to the following form, \n","\n","$$\\begin{array}{l}\n","\\min _{\\beta, \\beta_{0}} \\frac{1}{2}\\|\\beta\\|^{2}+C \\sum_{i=1}^{N} \\xi_{i} \\\\\n","\\text { subject to } \\xi_{i} \\geq 0, y_{i}\\left(x_{i}^{T} \\beta+\\beta_{0}\\right) \\geq 1-\\xi_{i} \\forall i,\n","\\end{array}$$\n","\n","In the training phase, Larger C results in the narrow margin (for infinitely large C the SVM becomes hard margin) and smaller C results in the wider margin.\n","\n","Now, we are going to minimize the following objective function shown below: \n","\n","$$L(\\beta) =\\frac{1}{2}\\|\\beta\\|^{2} + \\frac{C}{N} \\sum_{i=1}^{N} \\operatorname{max}(0, 1- y_{i}\\left(x_{i}^{T} \\beta+\\beta_{0}\\right) )$$"]},{"cell_type":"code","metadata":{"id":"14VImUJ3BJmv","executionInfo":{"status":"ok","timestamp":1649650940069,"user_tz":-480,"elapsed":7,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["# set hyper-parameters and call init\n","regularization_strength = 10000\n","learning_rate = 0.000001\n","class SVM_classifier:\n","    \n","    def __init__(self):\n","        return\n","    \n","    def fit(self, x, y):\n","        self.W = self._sgd(x.to_numpy(), y.to_numpy())\n","        print(\"training finished.\")\n","        print(\"weights are: {}\".format(self.W))\n","        return self\n","    def _compute_cost(self, W, X, Y):\n","        # calculate hinge loss\n","        N = X.shape[0]\n","        distances = 1 - Y * (np.dot(X, W))\n","        distances[distances < 0] = 0  # equivalent to max(0, distance)\n","        hinge_loss = regularization_strength * (np.sum(distances) / N)\n","\n","        # calculate cost\n","        cost = 1 / 2 * np.dot(W, W) + hinge_loss\n","        return cost\n","    def _calculate_cost_gradient(self, W, X_batch, Y_batch):\n","        # if only one example is passed (eg. in case of SGD)\n","        if type(Y_batch) == np.float64:\n","            Y_batch = np.array([Y_batch])\n","            X_batch = np.array([X_batch])  # gives multidimensional array\n","\n","        distance = 1 - (Y_batch * np.dot(X_batch, W))\n","        dw = np.zeros(len(W))\n","\n","        for ind, d in enumerate(distance):\n","            if max(0, d) == 0:\n","                di = W\n","            else:\n","                di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n","            dw += di\n","\n","        dw = dw/len(Y_batch)  # average\n","        return dw\n","    def _sgd(self, features, outputs):\n","        \"\"\"features and outputs need to be numpy array\"\"\"\n","        max_epochs = 5000\n","        weights = np.zeros(features.shape[1])\n","        nth = 0\n","        prev_cost = float(\"inf\")\n","        cost_threshold = 0.01  # in percent\n","        # stochastic gradient descent\n","        for epoch in range(1, max_epochs):\n","            # shuffle to prevent repeating update cycles\n","            X, Y = shuffle(features, outputs)\n","            for ind, x in enumerate(X):\n","                ascent = self._calculate_cost_gradient(weights, x, Y[ind])\n","                weights = weights - (learning_rate * ascent)\n","\n","            # convergence check on 2^nth epoch\n","            if epoch == 2 ** nth or epoch == max_epochs - 1:\n","                cost = self._compute_cost(weights, features, outputs)\n","                print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n","                # stoppage criterion\n","                if abs(prev_cost - cost) < cost_threshold * prev_cost:\n","                    return weights\n","                prev_cost = cost\n","                nth += 1\n","        return weights"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hg564DRrBJm2"},"source":["## Prediction for new data\n","So far we have produced model parameters. To predict new classes given the pretrained model, we don't need to update the parameter. Thus, we need a new function ```prediction(...)```\n"]},{"cell_type":"code","metadata":{"id":"7fRZ8HiQBJm3","executionInfo":{"status":"ok","timestamp":1649650940070,"user_tz":-480,"elapsed":5,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["def predict(self, x):\n","    y_predicted = np.array([])\n","    for i in range(x.shape[0]):\n","        yp = np.sign(np.dot(x.to_numpy()[i], self.W))\n","        y_predicted = np.append(y_predicted, yp)\n","    return y_predicted\n","\n","SVM_classifier.predict = predict"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RH7Wy69TBJm6"},"source":["## Experiment\n","Next, we’ll be working with a breast cancer dataset available on [Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data). As before we only use two features for better visualization."]},{"cell_type":"code","metadata":{"id":"W5qIMv5PtdB3","executionInfo":{"status":"ok","timestamp":1649650940396,"user_tz":-480,"elapsed":330,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["import statsmodels.api as sm\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split as tts\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from sklearn.utils import shuffle\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCre1KiCBJm6","executionInfo":{"status":"ok","timestamp":1649650940397,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["\n","# >> FEATURE SELECTION << #\n","def remove_correlated_features(X):\n","    corr_threshold = 0.9\n","    corr = X.corr()\n","    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n","    for i in range(corr.shape[0]):\n","        for j in range(i + 1, corr.shape[0]):\n","            if corr.iloc[i, j] >= corr_threshold:\n","                drop_columns[j] = True\n","    columns_dropped = X.columns[drop_columns]\n","    X.drop(columns_dropped, axis=1, inplace=True)\n","    return columns_dropped\n","\n","\n","def remove_less_significant_features(X, Y):\n","    sl = 0.05\n","    regression_ols = None\n","    columns_dropped = np.array([])\n","    for itr in range(0, len(X.columns)):\n","        regression_ols = sm.OLS(Y, X).fit()\n","        max_col = regression_ols.pvalues.idxmax()\n","        max_val = regression_ols.pvalues.max()\n","        if max_val > sl:\n","            X.drop(max_col, axis='columns', inplace=True)\n","            columns_dropped = np.append(columns_dropped, [max_col])\n","        else:\n","            break\n","    regression_ols.summary()\n","    return columns_dropped"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nf7JNcJQuuGw","outputId":"3797294a-32f6-4932-db5d-1a85c14d29b4","executionInfo":{"status":"ok","timestamp":1649650940398,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["print(\"reading dataset...\")\n","# read data in pandas (pd) data frame\n","data = pd.read_csv('./data.csv')\n","\n","# drop last column (extra column added by pd)\n","# and unnecessary first column (id)\n","data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n","\n","print(\"applying feature engineering...\")\n","# convert categorical labels to numbers\n","diag_map = {'M': 1.0, 'B': -1.0}\n","data['diagnosis'] = data['diagnosis'].map(diag_map)\n","\n","# put features & outputs in different data frames\n","Y = data.loc[:, 'diagnosis']\n","X = data.iloc[:, 1:]\n","\n","# filter features\n","remove_correlated_features(X)\n","remove_less_significant_features(X, Y)\n","\n","# normalize data for better convergence and to prevent overflow\n","X_normalized = MinMaxScaler().fit_transform(X.values)\n","X = pd.DataFrame(X_normalized)\n","\n","# insert 1 in every row for intercept b\n","X.insert(loc=len(X.columns), column='intercept', value=1)\n","\n","# split data into train and test set\n","print(\"splitting dataset into train and test sets...\")\n","X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["reading dataset...\n","applying feature engineering...\n","splitting dataset into train and test sets...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfyqOgQQvGNz","outputId":"1ec22c8e-0c69-4b10-d1ba-a14edf6c334e","executionInfo":{"status":"ok","timestamp":1649650955986,"user_tz":-480,"elapsed":15599,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["model = SVM_classifier()\n","model.fit(X_train, y_train)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch is: 1 and Cost is: 7226.631781243718\n","Epoch is: 2 and Cost is: 6718.745279355554\n","Epoch is: 4 and Cost is: 5543.138445903449\n","Epoch is: 8 and Cost is: 3850.3110958451207\n","Epoch is: 16 and Cost is: 2630.836909563661\n","Epoch is: 32 and Cost is: 1959.463652259652\n","Epoch is: 64 and Cost is: 1593.6849558404429\n","Epoch is: 128 and Cost is: 1324.782791125777\n","Epoch is: 256 and Cost is: 1158.6017735548542\n","Epoch is: 512 and Cost is: 1080.3310531551897\n","Epoch is: 1024 and Cost is: 1049.5975862744087\n","Epoch is: 2048 and Cost is: 1041.6398680490345\n","training finished.\n","weights are: [ 3.55394667 11.03426046 -2.30419309 -7.89882843 10.15727646 -1.27468922\n"," -6.4398698   2.25059307 -3.88065271  3.22811997  4.94187004  4.8266097\n"," -4.70381503]\n"]},{"output_type":"execute_result","data":{"text/plain":["<__main__.SVM_classifier at 0x7f13f4550e90>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"XLdesgBIwb74","executionInfo":{"status":"ok","timestamp":1649650955987,"user_tz":-480,"elapsed":134,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["# prediction \n","y_test_predicted = model.predict(X_test)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fR3k6uavCsD","outputId":"4c0bff87-2a89-413f-af6d-161ed282ebe3","executionInfo":{"status":"ok","timestamp":1649650955988,"user_tz":-480,"elapsed":133,"user":{"displayName":"Chen Liu","userId":"03707558357552264570"}}},"source":["print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n","print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n","print(\"precision on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy on test dataset: 0.9912280701754386\n","recall on test dataset: 0.9767441860465116\n","precision on test dataset: 0.9767441860465116\n"]}]},{"cell_type":"markdown","metadata":{"id":"iTodAw0ixEes"},"source":["# Bonus Question: \n","Why do we remove one of the correlated features? There are multiple reasons but the simplest of them is that correlated features almost have the same effect on the dependent variable. Moreover, correlated features won’t improve our model and would most probably worsen it, therefore we are better off using only one of them. After all, fewer features result in improved learning speed and a simpler model (model with fewer features).\n","\n"]}]}