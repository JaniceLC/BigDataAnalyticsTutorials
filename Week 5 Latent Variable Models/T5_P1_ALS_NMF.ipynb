{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T5_P1_ALS_NMF.ipynb","provenance":[{"file_id":"1TAAm68mJpB8UV0y6TlYCrPNlT696yXC7","timestamp":1647240007972}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HXf7Lorp9MqT"},"source":["# Non-negative Matrix Factorization using Alternating Least Squares\n","## Formulation\n","- Matrix Factorization (MF)\n","MF is a very commonly used technique in data compression, dimension reduction, clustering, and feature learning. Given a matrix $A \\in \\mathbb{R}^{m \\times n}$, our task is to find a low rank approximation to $A$. The most popular formation is\n","$$\n","\\min _{H \\in \\mathbb{R}^{m \\times k}, W \\in \\mathbb{R}^{n \\times k}} f(H, W):=\\frac{1}{2}\\left\\|A-H W^{T}\\right\\|_{F}^{2} .\n","$$\n","If $A$ is the data matrix (each column is a data sample), then $H$ is usually explained as the pattern matrix and $W$ is the coefficient matrix. This problem in (1) is not convex but its global optimal solution can be obtained. \n","\n","\n","- Alternating Least Squares \n","1.1 Alternating Least Square method (ALS)\n","The ALS algorithm is a special case of the block coordinate descent algorithm. The key motivation behind of ALS is to iteratively optimize a single variable or a single group variable while fix the rest. To solve (1), we repeatedly solve $H$ fixing $W$ first and then solve $W$ fixing $H$. First we fix $W$ to solve $H$ by\n","$$\n","\\min _{H \\in \\mathbb{R}^{m \\times k}} \\frac{1}{2}\\left\\|A-H W^{T}\\right\\|_{F}^{2} .\n","$$\n","This is a least squares problem. (We can solve it using the algorithms we learned in our last class.) Here, we give a closed form solution to it. To let the gradient be zero, we have\n","$$\n","\\left(A-H W^{T}\\right) W=0\n","$$\n","\n","\n","Similarly, we can solve $W$ by fixing $H$. We can have the closed form solution vt solving\n","$$\n","\\left(A-H W^{T}\\right)^{T} H=0 .\n","$$\n","\n","The complexity of each iteration is $O(m n r+$ $\\left.m r^{2}+n r^{2}\\right)$.\n","\n","\n","----\n","\n","## Extension\n","Further, we could regularize $H$ and $W$ using the L2 penalty. \n","$$\n","\\min _{H \\in \\mathbb{R}^{m \\times r}, W \\in \\mathbb{R}^{n \\times r}} f(H, W):=\\frac{1}{2}\\left\\|A-H W^{T}\\right\\|_{F}^{2} + \\lambda (\\left\\| H \\right\\|^2 + \\left\\| W \\right\\|^2) .\n","$$\n","\n","Alternating Least Squares (ALS) \n","1. Initialize the user vectors $H$ somehow (e.g. randomly).\n","\n","2. If we use closed form solutions, repeate (a) and (b) until desired level of convergence is achieved. \n","\n","    (a) For each item i, let $A_{\\cdot i}$ be the vector of ratings of that item (it will have m_users components; one for each user). Compute\n","    $$\n","    W_{i\\cdot}=\\left(H^{T} H+\\lambda I\\right)^{-1} H^{T} A_{\\cdot i}\n","    $$\n","    for each item i. \n","\n","    (b) For each user u, let $A_{u \\cdot}$ be the vector of ratings of that user (it will have n_items components; one for each item). Compute\n","    $$\n","    H_{u\\cdot}=\\left(W^{T} W+\\lambda I\\right)^{-1} W^{T} A_{u \\cdot}\n","    $$\n","    for each user $\\mathrm{u}$.\n","\n","----\n","Alternatively, (a) and (b) could be substituted by the least-squares solution to a linear matrix equation. \n","\n"]},{"cell_type":"code","metadata":{"id":"WCLq2q0Eyco7","executionInfo":{"status":"ok","timestamp":1647244472349,"user_tz":-480,"elapsed":16,"user":{"displayName":"Chen Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM2ipqYmgeRL5cOZwxm-gxkYqSorfdefrd-jK0Qw=s64","userId":"03707558357552264570"}}},"source":["import numpy as np\n","from numpy.linalg import lstsq\n","import pandas as pd\n","\n","\n","def ALS_NMF(A, k, num_iter, init_H = None, init_W = None, print_enabled = True):\n","\t'''\n","\tRun the alternating least squares method to perform nonnegative matrix factorization on A.\n","\tReturn matrices W, H such that A = WH.\n","\t\n","\tParameters:\n","\t\tA: ndarray\n","\t\t\t- m by n matrix to factorize\n","\t\tk: int\n","\t\t\t- integer specifying the column length of W / the row length of H\n","\t\t\t- the resulting matrices W, H will have sizes of m by k and k by n, respectively\n","\t\tnum_iter: int\n","\t\t\t- number of iterations for the multiplicative updates algorithm\n","\t\tprint_enabled: boolean\n","\t\t\t- if ture, output print statements\n","\n","\tReturns:\n","\t\tH: ndarray\n","\t\t\t- m by k matrix where k = dim\n","\t\tW: ndarray\n","\t\t\t- n by k matrix where k = dim\n","\t\t\n","\t'''\n","\n","\tprint('Applying the alternating least squares method on the input matrix...')\n","\n","\tif print_enabled:\n","\t\tprint('---------------------------------------------------------------------')\n","\t\tprint('Frobenius norm ||A - HW^{T}||_F')\n","\t\tprint('')\n","\n","\t# Initialize H and W\n","\n","\tif init_H is None:\n","\t\tH = np.random.rand(np.size(A, 0), k )\n","\telse:\n","\t\t# you may check the shape of init_H before assignment \n","\t\tH = init_H\n","\n","\tif init_W is None:\n","\t\tW = np.random.rand(np.size(A, 1), k)\n","\telse:\n","\t\t# you may check the shape of init_W before assignment \n","\t\tW = init_W\n","\n","\n","\t# Decompose the input matrix\n","\tfor n in range(num_iter):\n","\t\t# Update W\n","\t\t# Solve the least squares problem: argmin_W^{T} ||HW^{T} - A||\n","\t\tW = lstsq(H, A, rcond = -1)[0].T\n","\t\t# Set negative elements of H to 0\n","\t\tW[W < 0] = 0\n","\n","\t    # Update H\n","\t\t# Solve the least squares problem: argmin_H.T ||WH^{T} - A.T||\n","\t\tH = lstsq(W, A.T, rcond = -1)[0].T\n","\n","\t\t# Set negative elements of W to 0\n","\t\tH[H < 0] = 0\n","\n","\t\tif print_enabled:\n","\t\t\tfrob_norm = np.linalg.norm(A - H @ W.T, 'fro')\n","\t\t\tprint(\"iteration \" + str(n + 1) + \": \" + str(frob_norm))\n","\n","\treturn H, W"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SEheiBTWy3m2","outputId":"8bf1f371-17a7-413d-8fa8-739bc756579d","executionInfo":{"status":"ok","timestamp":1647244473168,"user_tz":-480,"elapsed":48,"user":{"displayName":"Chen Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM2ipqYmgeRL5cOZwxm-gxkYqSorfdefrd-jK0Qw=s64","userId":"03707558357552264570"}}},"source":["d = {'item0': [np.nan, 7, 6, 1, 1], 'item1': [3, 6, 7, 2, np.nan], \n","      'item2': [3, 7, np.nan, 2, 1], 'item3': [1, 4, 4, 3, 2],\n","      'item4': [1, 5, 3, 3, 3], 'item5': [np.nan, 4, 4, 4, 3]}\n","df = pd.DataFrame(data=d).astype('float32')\n","\n","X = np.array(df)\n","missing_idx = np.isnan(X)\n","\n","X1 = X.copy()\n","X1[missing_idx] = 2\n","\n","H, W = ALS_NMF(X1, k = 2, num_iter= 15)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Applying the alternating least squares method on the input matrix...\n","---------------------------------------------------------------------\n","Frobenius norm ||A - HW^{T}||_F\n","\n","iteration 1: 5.202488752790232\n","iteration 2: 4.191993431343542\n","iteration 3: 4.113824059999216\n","iteration 4: 4.058007813906313\n","iteration 5: 4.013112225139666\n","iteration 6: 3.9802648988460585\n","iteration 7: 3.9579683344249985\n","iteration 8: 3.943595428406581\n","iteration 9: 3.9346366594549123\n","iteration 10: 3.9291691690889077\n","iteration 11: 3.9258752708494726\n","iteration 12: 3.9239063028768593\n","iteration 13: 3.9227348223534477\n","iteration 14: 3.9220397640407048\n","iteration 15: 3.921628056416324\n"]}]},{"cell_type":"markdown","metadata":{"id":"vMx-edP1BkJC"},"source":["# How to deal with missing data in $R$? \n"]},{"cell_type":"code","metadata":{"id":"lw4aOXBrxpvJ","executionInfo":{"status":"ok","timestamp":1647244481880,"user_tz":-480,"elapsed":9,"user":{"displayName":"Chen Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM2ipqYmgeRL5cOZwxm-gxkYqSorfdefrd-jK0Qw=s64","userId":"03707558357552264570"}}},"source":["import numpy as np\n","from numpy.linalg import lstsq\n","import pandas as pd\n","from numpy.linalg import lstsq\n","\n","\n","def ALS_NMF_missing(A, k, max_iter = 100, eps = 0.05, init_H = None, init_W = None, print_enabled = True):\n","\t'''\n","\tRun the alternating least squares method to perform nonnegative matrix factorization on A.\n","\tReturn matrices W, H such that A = WH.\n","\t\n","\tParameters:\n","\t\tA: ndarray\n","\t\t\t- m by n matrix to factorize\n","\t\tk: int\n","\t\t\t- integer specifying the column length of W / the row length of H\n","\t\t\t- the resulting matrices W, H will have sizes of m by k and k by n, respectively\n","\t\tnum_iter: int\n","\t\t\t- number of iterations for the multiplicative updates algorithm\n","\t\tprint_enabled: boolean\n","\t\t\t- if ture, output print statements\n","\n","\tReturns:\n","\t\tH: ndarray\n","\t\t\t- m by k matrix where k = dim\n","\t\tW: ndarray\n","\t\t\t- n by k matrix where k = dim\n","\t\t\n","\t'''\n","\n","\tprint('Applying the alternating least squares method on the input matrix...')\n","\n","\tif print_enabled:\n","\t\tprint('---------------------------------------------------------------------')\n","\t\tprint('Frobenius norm ||A - HW^{T}||_F')\n","\t\tprint('')\n","\n","\t# Initialize H and W\n","\n","\tif init_H is None:\n","\t\tH = np.random.rand(np.size(A, 0), k )\n","\telse:\n","\t\t# you may check the shape of init_H before assignment \n","\t\tH = init_H\n","\n","\tif init_W is None:\n","\t\tW = np.random.rand(np.size(A, 1), k)\n","\telse:\n","\t\t# you may check the shape of init_W before assignment \n","\t\tW = init_W\n","\t\n","\tA_ori = A\n","\trowmean = np.nanmean(A, axis = 1)\n","\tinds = np.where(np.isnan(A)) \n","\t#Place row means in the indices. Align the arrays using take\n","\tA[inds] = np.take(rowmean, inds[0])\n","\t# Decompose the input matrix\n","\tA_est = np.matmul(H,W.T)\n","\tsq_error = np.square(np.subtract(A, A_est)) \n","\tsq_error[inds] = 0\n","\tloss_pre = np.sqrt(sq_error.sum())\n","\n","\t# Decompose the input matrix\n","\tdiff = 10000\n","\tcount = 0\n","\twhile diff > eps: \n","\t\tif count > max_iter: \n","\t\t\tprint('Maximum Iteration Reached!! ')\n","\t\t\tbreak \n","\t\t\t# Update W\n","\t\t\t# Solve the least squares problem: argmin_W^{T} ||HW^{T} - A||\n","\t\tW = lstsq(H, A, rcond = -1)[0].T\n","\t\t# Set negative elements of H to 0\n","\t\tW[W < 0] = 0\n","\n","\t\t# Update H\n","\t\t# Solve the least squares problem: argmin_H.T ||WH^{T} - A.T||\n","\t\tH = lstsq(W, A.T, rcond = -1)[0].T\n","\n","\t\t# Set negative elements of W to 0\n","\t\tH[H < 0] = 0\n","\n","\t\t### Calculate the squared error for only for the existing entries. \n","\t\tA_est = np.matmul(H,W.T)\n","\t\tsq_error = np.square(np.subtract(A, A_est)) \n","\t\t### ignore missing data\n","\t\tsq_error[inds] = 0\n","\t\tsq_error_sum = np.sqrt(sq_error.sum()) \n","\t\tif print_enabled:\n","\t\t\tprint(\"iteration \" + str(count + 1) + \": \" + str(sq_error_sum))\n","\t\tdiff = np.absolute(sq_error_sum - loss_pre)\n","\t\tloss_pre = sq_error_sum\n","\t\tA[inds] = A_est[inds]\n","\t\tcount += 1\n","\treturn H, W\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcYukJ141avy","outputId":"8bd8bcca-8531-4cae-b059-86402e35748e","executionInfo":{"status":"ok","timestamp":1647244486444,"user_tz":-480,"elapsed":418,"user":{"displayName":"Chen Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM2ipqYmgeRL5cOZwxm-gxkYqSorfdefrd-jK0Qw=s64","userId":"03707558357552264570"}}},"source":["\n","d = {'item0': [np.nan, 7, 6, 1, 1], 'item1': [3, 6, 7, 2, np.nan], \n","      'item2': [3, 7, np.nan, 2, 1], 'item3': [1, 4, 4, 3, 2],\n","      'item4': [1, 5, 3, 3, 3], 'item5': [np.nan, 4, 4, 4, 3]}\n","df = pd.DataFrame(data=d).astype('float32')\n","\n","X = np.array(df)\n","X\n","W, H  = ALS_NMF_missing(X, k=2, max_iter = 50, eps = 0.00005)\n"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Applying the alternating least squares method on the input matrix...\n","---------------------------------------------------------------------\n","Frobenius norm ||A - HW^{T}||_F\n","\n","iteration 1: 3.9691448846081516\n","iteration 2: 2.6617188771030116\n","iteration 3: 2.1282095489096986\n","iteration 4: 1.9047445523034296\n","iteration 5: 1.81876365332908\n","iteration 6: 1.7862897053781979\n","iteration 7: 1.773050979197988\n","iteration 8: 1.7672225102122554\n","iteration 9: 1.7645472884311624\n","iteration 10: 1.7632993243906623\n","iteration 11: 1.762714503612727\n","iteration 12: 1.7624403098407593\n","iteration 13: 1.7623118072752912\n","iteration 14: 1.7622515967805634\n","iteration 15: 1.7622233746822888\n"]}]},{"cell_type":"code","metadata":{"id":"AP6DR2T45jnv"},"source":[""],"execution_count":null,"outputs":[]}]}